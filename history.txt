import os
import json
import threading
import multiprocessing

from confluent_kafka import Producer


_requests_queue: multiprocessing.Queue = None
MODULE_NAME = os.getenv('MODULE_NAME')


def proceed_to_deliver(id, details):
    details['source'] = MODULE_NAME
    _requests_queue.put(details)


def producer_job(_, config, requests_queue: multiprocessing.Queue):
    producer = Producer(config)

    def delivery_callback(err, msg):
        if err:
            print('[error] Message failed delivery: {}'.format(err))

    topic = 'monitor'
    while True:
        event_details = requests_queue.get()
        producer.produce(
            topic,
            json.dumps(event_details),
            event_details['id'],
            callback=delivery_callback
        )

        producer.poll(10000)
        producer.flush()


def start_producer(args, config, requests_queue):
    print(f'{MODULE_NAME}_producer started')

    global _requests_queue

    _requests_queue = requests_queue
    threading.Thread(
        target=lambda: producer_job(args, config, requests_queue)
    ).start()


import os

from argparse import ArgumentParser, FileType
from configparser import ConfigParser
from multiprocessing import Queue

from .consumer import start_consumer
from .producer import start_producer


MODULE_NAME = os.getenv("MODULE_NAME")


def main():
    print(f"[DEBUG] {MODULE_NAME} started...")

    parser = ArgumentParser()
    parser.add_argument("config_file", type=FileType("r"))
    parser.add_argument("--reset", action="store_true")
    args = parser.parse_args()

    config_parser = ConfigParser()
    config_parser.read_file(args.config_file)
    config = dict(config_parser["default"])
    config.update(config_parser[MODULE_NAME])

    requests_queue = Queue()
    print(f"Running {MODULE_NAME}_consumer...")
    start_consumer(args, config)
    print(f"Running {MODULE_NAME}_producer...")
    start_producer(args, config, requests_queue)

